{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Persian Music Recommendation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_fQYklRms9FV",
        "outputId": "f8fab275-0b02-4d30-cea4-3bbde7a7b116"
      },
      "source": [
        "!pip install tomotopy\n",
        "!pip install Hazm"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tomotopy\n",
            "  Downloading tomotopy-0.12.2-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (16.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 16.3 MB 124 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tomotopy) (1.19.5)\n",
            "Installing collected packages: tomotopy\n",
            "Successfully installed tomotopy-0.12.2\n",
            "Collecting Hazm\n",
            "  Downloading hazm-0.7.0-py3-none-any.whl (316 kB)\n",
            "\u001b[K     |████████████████████████████████| 316 kB 31.0 MB/s \n",
            "\u001b[?25hCollecting nltk==3.3\n",
            "  Downloading nltk-3.3.0.zip (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 39.9 MB/s \n",
            "\u001b[?25hCollecting libwapiti>=0.2.1\n",
            "  Downloading libwapiti-0.2.1.tar.gz (233 kB)\n",
            "\u001b[K     |████████████████████████████████| 233 kB 46.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk==3.3->Hazm) (1.15.0)\n",
            "Building wheels for collected packages: nltk, libwapiti\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.3-py3-none-any.whl size=1394484 sha256=2ed77790be732799cda53ed4ba6d66d78ce4be7cf09c377cdf02067543f66c3a\n",
            "  Stored in directory: /root/.cache/pip/wheels/9b/fd/0c/d92302c876e5de87ebd7fc0979d82edb93e2d8d768bf71fac4\n",
            "  Building wheel for libwapiti (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for libwapiti: filename=libwapiti-0.2.1-cp37-cp37m-linux_x86_64.whl size=154352 sha256=c9b65aa7574b4eafc53ba1787cbf78546f2ed5fba3f068a1ca648b55ea058e77\n",
            "  Stored in directory: /root/.cache/pip/wheels/ab/b2/5b/0fe4b8f5c0e65341e8ea7bb3f4a6ebabfe8b1ac31322392dbf\n",
            "Successfully built nltk libwapiti\n",
            "Installing collected packages: nltk, libwapiti, Hazm\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed Hazm-0.7.0 libwapiti-0.2.1 nltk-3.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "keoU9Ypxx_vK",
        "outputId": "0e8b090c-5414-43f7-ee1e-5bbd3c816d2b"
      },
      "source": [
        "!wget 'https://cdn-129.anonfiles.com/X2N2X0G1u6/54c25b6f-1631379843/nlp_model.zip'\n",
        "!unzip \"nlp_model.zip\" -d \"\""
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-09-11 16:56:51--  https://cdn-129.anonfiles.com/X2N2X0G1u6/54c25b6f-1631379843/nlp_model.zip\n",
            "Resolving cdn-129.anonfiles.com (cdn-129.anonfiles.com)... 45.154.253.58, 2001:678:b30:5::b\n",
            "Connecting to cdn-129.anonfiles.com (cdn-129.anonfiles.com)|45.154.253.58|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 470805 (460K) [application/zip]\n",
            "Saving to: ‘nlp_model.zip’\n",
            "\n",
            "nlp_model.zip       100%[===================>] 459.77K   637KB/s    in 0.7s    \n",
            "\n",
            "2021-09-11 16:57:09 (637 KB/s) - ‘nlp_model.zip’ saved [470805/470805]\n",
            "\n",
            "Archive:  nlp_model.zip\n",
            "  inflating: lda_model.bin           \n",
            "  inflating: test_data.txt           \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtjYm42Vzbub"
      },
      "source": [
        "\n",
        "**Custom lyric for Test**\n",
        "\n",
        "For test your lyric, you can change the text of **test_data.txt**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V1ot9zFdtrvY",
        "outputId": "9cb89e23-43bf-49e2-e833-e28cdf8f638c"
      },
      "source": [
        "import hazm\n",
        "import tomotopy as tp\n",
        "\n",
        "\n",
        "\n",
        "class LDAModelTraining:\n",
        "    topic_counts = 3\n",
        "    word_topic_show = 15\n",
        "\n",
        "    topic_map = {0: \"عاشقانه\",\n",
        "                 1: \"گنگ و دیس\",\n",
        "                 2: \"مذهبی\"}\n",
        "\n",
        "    def __init__(self):\n",
        "        self.mdl = tp.LDAModel.load(\"lda_model.bin\")\n",
        "\n",
        "    def test(self, title: str, lyric: str):\n",
        "        doc = []\n",
        "        lyric = lyric.split(\"\\n\")\n",
        "        for line in lyric:\n",
        "            for word in line.strip().split():\n",
        "                doc.append(word)\n",
        "\n",
        "        doc_inst = self.mdl.make_doc(doc)\n",
        "        topic_dist, ll = self.mdl.infer(doc_inst)\n",
        "        best_topic_index = list(topic_dist).index(max(topic_dist))\n",
        "        best_topic_word_list = self.mdl.get_topic_words(best_topic_index, top_n=self.word_topic_show)\n",
        "        print(title)\n",
        "        print(f\"Best Topic: {self.get_words_of_topic(best_topic_word_list)}\")\n",
        "        print(f\"topic: {self.topic_map[best_topic_index]}\")\n",
        "        print(f\"percentage: {round(max(topic_dist), 2)}\")\n",
        "        print(\"---------------------------------------------------------------------------------\")\n",
        "\n",
        "    def get_words_of_topic(self, topic_words):\n",
        "        words = []\n",
        "        for map in topic_words:\n",
        "            words.append(map[0])\n",
        "\n",
        "        return words\n",
        "\n",
        "    def fetch_data_test(self):\n",
        "\n",
        "        normalizer = hazm.Normalizer()\n",
        "\n",
        "        title = \"test lyric\"\n",
        "        lyric = normalizer.normalize(open(\"test_data.txt\").read())\n",
        "        self.test(title, lyric)\n",
        "\n",
        "    def print_topics(self):\n",
        "        for i in range(0, 3):\n",
        "            print(i, LDAModelTraining().get_words_of_topic(self.mdl.get_topic_words(i, top_n=self.word_topic_show)))\n",
        "\n",
        "\n",
        "LDAModelTraining().fetch_data_test()\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test lyric\n",
            "Best Topic: ['دل', 'تو', 'واسه', 'عشق', 'قلب', 'دست', 'شب', 'دنیا', 'دوست', 'فکر', 'برو', 'آمد', 'ببین', 'منی', 'بری']\n",
            "topic: عاشقانه\n",
            "percentage: 0.9800000190734863\n",
            "---------------------------------------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}