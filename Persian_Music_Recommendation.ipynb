{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Persian Music Recommendation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPxe0WE13uR3qkM2mnr6IX3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohammadaminabbasi/Darkflow-NLP-Persian-Music-Recommendation-on-Lyrics/blob/main/Persian_Music_Recommendation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_fQYklRms9FV",
        "outputId": "cccdd190-218e-478f-8ff4-157032fbc845"
      },
      "source": [
        "!pip install tomotopy\n",
        "!pip install Hazm"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tomotopy\n",
            "  Downloading tomotopy-0.12.2-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (16.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 16.3 MB 135 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tomotopy) (1.19.5)\n",
            "Installing collected packages: tomotopy\n",
            "Successfully installed tomotopy-0.12.2\n",
            "Collecting Hazm\n",
            "  Downloading hazm-0.7.0-py3-none-any.whl (316 kB)\n",
            "\u001b[K     |████████████████████████████████| 316 kB 5.0 MB/s \n",
            "\u001b[?25hCollecting nltk==3.3\n",
            "  Downloading nltk-3.3.0.zip (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 38.3 MB/s \n",
            "\u001b[?25hCollecting libwapiti>=0.2.1\n",
            "  Downloading libwapiti-0.2.1.tar.gz (233 kB)\n",
            "\u001b[K     |████████████████████████████████| 233 kB 64.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk==3.3->Hazm) (1.15.0)\n",
            "Building wheels for collected packages: nltk, libwapiti\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.3-py3-none-any.whl size=1394485 sha256=d71604db398f4de29144622acd1780c05c20e9b3a72ecda155197b187f6e5466\n",
            "  Stored in directory: /root/.cache/pip/wheels/9b/fd/0c/d92302c876e5de87ebd7fc0979d82edb93e2d8d768bf71fac4\n",
            "  Building wheel for libwapiti (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for libwapiti: filename=libwapiti-0.2.1-cp37-cp37m-linux_x86_64.whl size=154361 sha256=f10baf2e7571fc00c3bc0c0a6414dffd139bba3cb37dda6e74ea1c8431d9cfc2\n",
            "  Stored in directory: /root/.cache/pip/wheels/ab/b2/5b/0fe4b8f5c0e65341e8ea7bb3f4a6ebabfe8b1ac31322392dbf\n",
            "Successfully built nltk libwapiti\n",
            "Installing collected packages: nltk, libwapiti, Hazm\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed Hazm-0.7.0 libwapiti-0.2.1 nltk-3.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "keoU9Ypxx_vK",
        "outputId": "28b74543-3e62-46f9-b049-501f700105ab"
      },
      "source": [
        "!wget 'https://filebin.net/0nxu25kv8qjg1rzr/lda_model.bin'\n",
        "!unzip \"nlp_model.zip\" -d \"\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-09-13 08:01:34--  https://filebin.net/n8f7x967dn1jkd17/nlp_model.zip\n",
            "Resolving filebin.net (filebin.net)... 185.47.40.36, 2a02:c0:2f0:700:f816:3eff:feac:c605\n",
            "Connecting to filebin.net (filebin.net)|185.47.40.36|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://situla.bitbit.net/filebin/7faa06f730bccfc6564573f8587739ad5b9cd7f224f0db2a63956a5710daa415/c37e05184731906ec996f60178e6a3fb2e5b9c940cca53484f1e2f34bb93806f?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=HZXB1J7T0UN34UN512IW%2F20210913%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20210913T080134Z&X-Amz-Expires=30&X-Amz-SignedHeaders=host&response-cache-control=max-age%3D30&response-content-disposition=filename%3D%22nlp_model.zip%22&response-content-type=application%2Fzip&X-Amz-Signature=94c7cbc9d984ec98b5efc9fc388cbcab7d56594f40455bd13183e03d29213e6b [following]\n",
            "--2021-09-13 08:01:34--  https://situla.bitbit.net/filebin/7faa06f730bccfc6564573f8587739ad5b9cd7f224f0db2a63956a5710daa415/c37e05184731906ec996f60178e6a3fb2e5b9c940cca53484f1e2f34bb93806f?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=HZXB1J7T0UN34UN512IW%2F20210913%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20210913T080134Z&X-Amz-Expires=30&X-Amz-SignedHeaders=host&response-cache-control=max-age%3D30&response-content-disposition=filename%3D%22nlp_model.zip%22&response-content-type=application%2Fzip&X-Amz-Signature=94c7cbc9d984ec98b5efc9fc388cbcab7d56594f40455bd13183e03d29213e6b\n",
            "Resolving situla.bitbit.net (situla.bitbit.net)... 87.238.33.7, 87.238.33.8, 2a02:c0::8, ...\n",
            "Connecting to situla.bitbit.net (situla.bitbit.net)|87.238.33.7|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 470805 (460K) [application/zip]\n",
            "Saving to: ‘nlp_model.zip’\n",
            "\n",
            "nlp_model.zip       100%[===================>] 459.77K   826KB/s    in 0.6s    \n",
            "\n",
            "2021-09-13 08:01:36 (826 KB/s) - ‘nlp_model.zip’ saved [470805/470805]\n",
            "\n",
            "Archive:  nlp_model.zip\n",
            "  inflating: lda_model.bin           \n",
            "  inflating: test_data.txt           \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtjYm42Vzbub"
      },
      "source": [
        "\n",
        "**Custom lyric for Test**\n",
        "\n",
        "For test your lyric, you can change the text of **test_data.txt**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V1ot9zFdtrvY",
        "outputId": "f701519f-55da-4ffa-e84d-00722b97763b"
      },
      "source": [
        "import hazm\n",
        "import tomotopy as tp\n",
        "\n",
        "\n",
        "\n",
        "class LDAModelTraining:\n",
        "    topic_counts = 3\n",
        "    word_topic_show = 15\n",
        "\n",
        "    topic_map = {0: \"عاشقانه\",\n",
        "                 1: \"گنگ و دیس\",\n",
        "                 2: \"مذهبی\"}\n",
        "\n",
        "    def __init__(self):\n",
        "        self.mdl = tp.LDAModel.load(\"lda_model.bin\")\n",
        "\n",
        "    def test(self, title: str, lyric: str):\n",
        "        doc = []\n",
        "        lyric = lyric.split(\"\\n\")\n",
        "        for line in lyric:\n",
        "            for word in line.strip().split():\n",
        "                doc.append(word)\n",
        "\n",
        "        doc_inst = self.mdl.make_doc(doc)\n",
        "        topic_dist, ll = self.mdl.infer(doc_inst)\n",
        "        best_topic_index = list(topic_dist).index(max(topic_dist))\n",
        "        best_topic_word_list = self.mdl.get_topic_words(best_topic_index, top_n=self.word_topic_show)\n",
        "        print(title)\n",
        "        print(f\"Top Words of Best Topic: {self.get_words_of_topic(best_topic_word_list)}\")\n",
        "        print(f\"topic: {self.topic_map[best_topic_index]}\")\n",
        "        print(f\"Possibility Percentage: {round(max(topic_dist), 2)}\")\n",
        "        print(\"---------------------------------------------------------------------------------\")\n",
        "\n",
        "    def get_words_of_topic(self, topic_words):\n",
        "        words = []\n",
        "        for map in topic_words:\n",
        "            words.append(map[0])\n",
        "\n",
        "        return words\n",
        "\n",
        "    def fetch_data_test(self):\n",
        "\n",
        "        normalizer = hazm.Normalizer()\n",
        "\n",
        "        title = \"test lyric\"\n",
        "        lyric = normalizer.normalize(open(\"test_data.txt\").read())\n",
        "        self.test(title, lyric)\n",
        "\n",
        "    def print_topics(self):\n",
        "        for i in range(0, 3):\n",
        "            print(i, LDAModelTraining().get_words_of_topic(self.mdl.get_topic_words(i, top_n=self.word_topic_show)))\n",
        "\n",
        "\n",
        "LDAModelTraining().fetch_data_test()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test lyric\n",
            "Top Words of Best Topic: ['آمد', 'گفت', 'تو', 'کار', 'واسه', 'میده', 'میگه', 'بده', 'برو', 'حاجی', 'بابا', 'شهر', 'مث', 'تهران', 'بازی']\n",
            "topic: گنگ و دیس\n",
            "Possibility Percentage: 0.41999998688697815\n",
            "---------------------------------------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}