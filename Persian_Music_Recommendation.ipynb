{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Persian Music Recommendation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM/9X/6H5pebiedUyRKqGvR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohammadaminabbasi/Darkflow-NLP-Persian-Music-Recommendation-on-Lyrics/blob/main/Persian_Music_Recommendation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_fQYklRms9FV",
        "outputId": "4513d719-71d1-4c5e-b9b5-ec0ca4397a27"
      },
      "source": [
        "!pip install tomotopy\n",
        "!pip install Hazm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tomotopy in /usr/local/lib/python3.7/dist-packages (0.12.2)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tomotopy) (1.19.5)\n",
            "Requirement already satisfied: Hazm in /usr/local/lib/python3.7/dist-packages (0.7.0)\n",
            "Requirement already satisfied: nltk==3.3 in /usr/local/lib/python3.7/dist-packages (from Hazm) (3.3)\n",
            "Requirement already satisfied: libwapiti>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from Hazm) (0.2.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk==3.3->Hazm) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "keoU9Ypxx_vK",
        "outputId": "6fcb93b2-2e3d-4088-c26d-9c66fae90d23"
      },
      "source": [
        "!wget 'https://filebin.net/n8f7x967dn1jkd17/nlp_model.zip'\n",
        "!unzip \"nlp_model.zip\" -d \"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  nlp_model.zip\n",
            "  inflating: lda_model.bin           \n",
            "  inflating: test_data.txt           \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtjYm42Vzbub"
      },
      "source": [
        "\n",
        "**Custom lyric for Test**\n",
        "\n",
        "For test your lyric, you can change the text of **test_data.txt**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V1ot9zFdtrvY",
        "outputId": "7b1ba10b-4ea2-46db-90ff-205a270829f8"
      },
      "source": [
        "import hazm\n",
        "import tomotopy as tp\n",
        "\n",
        "\n",
        "\n",
        "class LDAModelTraining:\n",
        "    topic_counts = 3\n",
        "    word_topic_show = 15\n",
        "\n",
        "    topic_map = {0: \"عاشقانه\",\n",
        "                 1: \"گنگ و دیس\",\n",
        "                 2: \"مذهبی\"}\n",
        "\n",
        "    def __init__(self):\n",
        "        self.mdl = tp.LDAModel.load(\"lda_model.bin\")\n",
        "\n",
        "    def test(self, title: str, lyric: str):\n",
        "        doc = []\n",
        "        lyric = lyric.split(\"\\n\")\n",
        "        for line in lyric:\n",
        "            for word in line.strip().split():\n",
        "                doc.append(word)\n",
        "\n",
        "        doc_inst = self.mdl.make_doc(doc)\n",
        "        topic_dist, ll = self.mdl.infer(doc_inst)\n",
        "        best_topic_index = list(topic_dist).index(max(topic_dist))\n",
        "        best_topic_word_list = self.mdl.get_topic_words(best_topic_index, top_n=self.word_topic_show)\n",
        "        print(title)\n",
        "        print(f\"Top Words of Best Topic: {self.get_words_of_topic(best_topic_word_list)}\")\n",
        "        print(f\"topic: {self.topic_map[best_topic_index]}\")\n",
        "        print(f\"Possibility Percentage: {round(max(topic_dist), 2)}\")\n",
        "        print(\"---------------------------------------------------------------------------------\")\n",
        "\n",
        "    def get_words_of_topic(self, topic_words):\n",
        "        words = []\n",
        "        for map in topic_words:\n",
        "            words.append(map[0])\n",
        "\n",
        "        return words\n",
        "\n",
        "    def fetch_data_test(self):\n",
        "\n",
        "        normalizer = hazm.Normalizer()\n",
        "\n",
        "        title = \"test lyric\"\n",
        "        lyric = normalizer.normalize(open(\"test_data.txt\").read())\n",
        "        self.test(title, lyric)\n",
        "\n",
        "    def print_topics(self):\n",
        "        for i in range(0, 3):\n",
        "            print(i, LDAModelTraining().get_words_of_topic(self.mdl.get_topic_words(i, top_n=self.word_topic_show)))\n",
        "\n",
        "\n",
        "LDAModelTraining().fetch_data_test()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test lyric\n",
            "Best Topic: ['دل', 'تو', 'واسه', 'عشق', 'قلب', 'دست', 'شب', 'دنیا', 'دوست', 'فکر', 'برو', 'آمد', 'ببین', 'منی', 'بری']\n",
            "topic: عاشقانه\n",
            "percentage: 0.9800000190734863\n",
            "---------------------------------------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}