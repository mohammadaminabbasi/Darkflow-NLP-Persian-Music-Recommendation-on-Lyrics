{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Persian Music Recommendation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOE4qJpcwdbPMZLB6ZyxkkD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohammadaminabbasi/Darkflow-NLP-Persian-Music-Recommendation-on-Lyrics/blob/main/Persian_Music_Recommendation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_fQYklRms9FV",
        "outputId": "4513d719-71d1-4c5e-b9b5-ec0ca4397a27"
      },
      "source": [
        "!pip install tomotopy\n",
        "!pip install Hazm"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tomotopy in /usr/local/lib/python3.7/dist-packages (0.12.2)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tomotopy) (1.19.5)\n",
            "Requirement already satisfied: Hazm in /usr/local/lib/python3.7/dist-packages (0.7.0)\n",
            "Requirement already satisfied: nltk==3.3 in /usr/local/lib/python3.7/dist-packages (from Hazm) (3.3)\n",
            "Requirement already satisfied: libwapiti>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from Hazm) (0.2.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk==3.3->Hazm) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "keoU9Ypxx_vK",
        "outputId": "6fcb93b2-2e3d-4088-c26d-9c66fae90d23"
      },
      "source": [
        "!wget 'https://filebin.net/n8f7x967dn1jkd17/nlp_model.zip'\n",
        "!unzip \"nlp_model.zip\" -d \"\""
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  nlp_model.zip\n",
            "  inflating: lda_model.bin           \n",
            "  inflating: test_data.txt           \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtjYm42Vzbub"
      },
      "source": [
        "\n",
        "**Custom lyric for Test**\n",
        "\n",
        "For test your lyric, you can change the text of **test_data.txt**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "id": "V1ot9zFdtrvY",
        "outputId": "b64ed3f7-d3c5-4682-9b8a-9d4ba5c6e3e1"
      },
      "source": [
        "import hazm\n",
        "import tomotopy as tp\n",
        "\n",
        "\n",
        "\n",
        "class LDAModelTraining:\n",
        "    topic_counts = 3\n",
        "    word_topic_show = 15\n",
        "\n",
        "    topic_map = {0: \"عاشقانه\",\n",
        "                 1: \"گنگ و دیس\",\n",
        "                 2: \"مذهبی\"}\n",
        "\n",
        "    def __init__(self):\n",
        "        self.mdl = tp.LDAModel.load(\"lda_model.bin\")\n",
        "\n",
        "    def test(self, title: str, lyric: str):\n",
        "        doc = []\n",
        "        lyric = lyric.split(\"\\n\")\n",
        "        for line in lyric:\n",
        "            for word in line.strip().split():\n",
        "                doc.append(word)\n",
        "\n",
        "        doc_inst = self.mdl.make_doc(doc)\n",
        "        topic_dist, ll = self.mdl.infer(doc_inst)\n",
        "        best_topic_index = list(topic_dist).index(max(topic_dist))\n",
        "        best_topic_word_list = self.mdl.get_topic_words(best_topic_index, top_n=self.word_topic_show)\n",
        "        print(title)\n",
        "        print(f\"Best Topic: {self.get_words_of_topic(best_topic_word_list)}\")\n",
        "        print(f\"topic: {self.topic_map[best_topic_index]}\")\n",
        "        print(f\"percentage: {round(max(topic_dist), 2)}\")\n",
        "        print(\"---------------------------------------------------------------------------------\")\n",
        "\n",
        "    def get_words_of_topic(self, topic_words):\n",
        "        words = []\n",
        "        for map in topic_words:\n",
        "            words.append(map[0])\n",
        "\n",
        "        return words\n",
        "\n",
        "    def fetch_data_test(self):\n",
        "\n",
        "        normalizer = hazm.Normalizer()\n",
        "\n",
        "        title = \"test lyric\"\n",
        "        lyric = normalizer.normalize(open(\"test_data.txt\").read())\n",
        "        self.test(title, lyric)\n",
        "\n",
        "    def print_topics(self):\n",
        "        for i in range(0, 3):\n",
        "            print(i, LDAModelTraining().get_words_of_topic(self.mdl.get_topic_words(i, top_n=self.word_topic_show)))\n",
        "\n",
        "\n",
        "LDAModelTraining().fetch_data_test()\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Exception",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-fcc948485708>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m \u001b[0mLDAModelTraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch_data_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-25-fcc948485708>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmdl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLDAModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"lda_model.bin\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlyric\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mException\u001b[0m: 'lda_model.bin' is not valid model file"
          ]
        }
      ]
    }
  ]
}